{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc359e4b",
   "metadata": {},
   "source": [
    "## Demo of implementing RAG\n",
    "This Jupyter notebook is for an initial implementation of RAG, following the tutorial from [Hugging Face](https://huggingface.co/blog/ngxson/make-your-own-rag)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd542ac",
   "metadata": {},
   "source": [
    "First we begin by loading our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba652e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 151 entries\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['On average, cats spend 2/3 of every day sleeping. That means a nine-year-old cat has been awake for only three years of its life.\\n',\n",
       " 'Unlike dogs, cats do not have a sweet tooth. Scientists believe this is due to a mutation in a key taste receptor.\\n',\n",
       " 'When a cat chases its prey, it keeps its head level. Dogs and humans bob their heads up and down.\\n',\n",
       " 'The technical term for a cat’s hairball is a “bezoar.”\\n',\n",
       " 'A group of cats is called a “clowder.”\\n']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "with open('cat-facts.txt', 'r') as file:\n",
    "  dataset = file.readlines()\n",
    "  print(f'Loaded {len(dataset)} entries')\n",
    "\n",
    "dataset[0:5]  # Display the first 5 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce06acd",
   "metadata": {},
   "source": [
    "## Implementing vector database\n",
    "\n",
    "We have to convert our plain text to vectors, to be able to use vector similarity search, rather than keyword search (not a viable alternative)\n",
    "<br><br/>\n",
    "To do this, we need to have selected the models we are going to work with. For convenience, we will use Ollama models, as they are free and can be run locally with minimal setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93dbddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f910456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "VECTOR_DB = []\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "  embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "  VECTOR_DB.append((chunk, embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65136e1",
   "metadata": {},
   "source": [
    "We will assume each line in our database to be one chunk. Let us now calculate the embeddings and also add them to the VECTOR_DB list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7274b98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 151 entries from vector_db.txt\n"
     ]
    }
   ],
   "source": [
    "# Let us check if the vectorization has already been done\n",
    "try:\n",
    "  with open('vector_db.txt', 'r') as file:\n",
    "    for line in file:\n",
    "      chunk, embedding_str = line.strip().split('\\t')\n",
    "      embedding = list(map(float, embedding_str.split(',')))\n",
    "      VECTOR_DB.append((chunk, embedding))\n",
    "  print(f'Loaded {len(VECTOR_DB)} entries from vector_db.txt')\n",
    "  vectorized_dataset_loaded = True\n",
    "except FileNotFoundError:\n",
    "  print('vector_db.txt not found, proceeding to vectorize the dataset')\n",
    "  for i, chunk in enumerate(dataset):\n",
    "    add_chunk_to_database(chunk)\n",
    "    print(f'Added chunk {i+1}/{len(dataset)} to the database')\n",
    "  vectorized_dataset_loaded = False\n",
    "    \n",
    "  print(f'Added {len(VECTOR_DB)} chunks to the database')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5350f",
   "metadata": {},
   "source": [
    "Let us save our vectorized database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f12ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors in this list of tuples are <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"The vectors in this list of tuples are {(type(VECTOR_DB[0][1]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d89b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vector database to a txt file\n",
    "if not vectorized_dataset_loaded:\n",
    "  print('Saving the vector database to vector_db.txt')\n",
    "  with open('vector_db.txt', 'w') as file:\n",
    "    for chunk, embedding in VECTOR_DB:\n",
    "      file.write(f\"{chunk.strip()}\\t{','.join(map(str, embedding))}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf58de",
   "metadata": {},
   "source": [
    "## Information Retrival\n",
    "Now we want to implement a function to retrieve information that is closest to the query for the LLM.<br>\n",
    "First we need to make a function that calculates cosine similarity. <br>\n",
    "Lets do it in a way that is fast (even though for this demo, we really don't need to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2dfcab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def cosine_sim_sklearn_vecs(a, b):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors using sklearn.\n",
    "    \"\"\"\n",
    "    a = np.array(a).reshape(1, -1)\n",
    "    b = np.array(b).reshape(1, -1)\n",
    "    return cosine_similarity(a, b)[0, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83869fb9",
   "metadata": {},
   "source": [
    "Now lets implement retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8bf730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "    # Calculate the embedding for the query\n",
    "    query_emb = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "    # Calculate cosine similarity for each chunk in the vector database\n",
    "    similarities = [(chunk, cosine_sim_sklearn_vecs(query_emb, emb)) for chunk, emb in VECTOR_DB]\n",
    "    # Sort the similarities in descending order\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05aae1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input query: I'm George. I have a pet called Muezza. What is my favorite type of cat?\n",
      "Retrieved knowledge:\n",
      " - (similarity: 0.79) Mohammed loved cats and reportedly his favorite cat, Muezza, was a tabby. Legend says that tabby cats have an “M” for Mohammed on top of their heads because Mohammad would often rest his hand on the cat’s head.\n",
      " - (similarity: 0.76) If you name is George, you are more likely to have parrots as pets. However, nonetheless, your favorite type of cat is probably going to be a persian cat.\n",
      " - (similarity: 0.67) The most popular pedigreed cat is the Persian cat, followed by the Main Coon cat and the Siamese cat.\n",
      "Retrieved knowledge:\n",
      " - (similarity: 0.79) Mohammed loved cats and reportedly his favorite cat, Muezza, was a tabby. Legend says that tabby cats have an “M” for Mohammed on top of their heads because Mohammad would often rest his hand on the cat’s head.\n",
      " - (similarity: 0.76) If you name is George, you are more likely to have parrots as pets. However, nonetheless, your favorite type of cat is probably going to be a persian cat.\n",
      " - (similarity: 0.67) The most popular pedigreed cat is the Persian cat, followed by the Main Coon cat and the Siamese cat.\n"
     ]
    }
   ],
   "source": [
    "# input_query = input('Ask me a question: ')\n",
    "input_query = \"I'm George. I have a pet called Muezza. What is my favorite type of cat?\"\n",
    "print(f'Input query: {input_query}')\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "    print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
    "\n",
    "instruction_prompt = (\n",
    "    \"You are a helpful chatbot.\\n\"\n",
    "    \"Use only the following pieces of context to answer the question. \"\n",
    "    \"Don't make up any new information:\\n\"\n",
    "    + \"\\n\".join([f' - {chunk}' for chunk, _ in retrieved_knowledge])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e70930",
   "metadata": {},
   "source": [
    "As you see, this simple approach of retrieval works relatively well, however, it can fail to bring up relevant information, especially if the chunks contain multiple piece of similar information.\n",
    "\n",
    "Notice that the input query states that the user is George, and there is information about Georges favorite cat in the 'database'. However, the first piece of information that is retrieved is about Mohammed, not Geroge.  <br> <br/>\n",
    "Let us see how the chatbot responds to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef232408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response:\n",
      "You are correct that your name is George, not Mohammed.\n",
      "\n",
      "Since you mentioned that Muezza is a tabby cat, it's likely that your favorite type of cat is a Persian cat. That's in line with the popular pedigreed cats mentioned: Persians, Main Coon cats, and Siamese cats.\n"
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "  model=LANGUAGE_MODEL,\n",
    "  messages=[\n",
    "    {'role': 'system', 'content': instruction_prompt},\n",
    "    {'role': 'user', 'content': input_query},\n",
    "  ],\n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "response = \"\"\n",
    "for chunk in stream:\n",
    "    response += chunk['message']['content']\n",
    "print(\"Chatbot response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ada22",
   "metadata": {},
   "source": [
    "## Improving Retrieval with Reranking\n",
    "While cosine similarity is a good first step, as mentioned, it can struggle with more information dense chunks. Even though, in this case, we still have gotten the correct answer (most of the time).\n",
    "\n",
    "To keep the efficiency of the cosine similarity approach, we wil still select the top 5 pieces of information using cosine similarity, and then rerank them to reorder and select the top 3 pieces of most relevant information. We can afford to use a more computationally demanding model for this step.\n",
    "\n",
    "We will use a cross-encoder reranker (e.g., from Hugging Face Transformers) to score each (query, chunk) pair and sort the results accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70ab0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0e8042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a cross-encoder reranker model (e.g., 'cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "# Actually, 'cross-encoder/ms-marco-MiniLM-L-6-v2' specifically does not work on M2 machines,\n",
    "# so we use 'cross-encoder/ms-marco-MiniLM-L-12-v2 (took a while to find out this information)\n",
    "reranker_model_name = 'cross-encoder/ms-marco-MiniLM-L-12-v2'\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)\n",
    "\n",
    "def rerank(query, retrieved_chunks, top_k=3):\n",
    "    pairs = [(query, chunk) for chunk, _ in retrieved_chunks]\n",
    "    inputs = reranker_tokenizer.batch_encode_plus(pairs, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**inputs).logits.squeeze(-1).tolist()\n",
    "    reranked = sorted(zip([chunk for chunk, _ in retrieved_chunks], scores), key=lambda x: x[1], reverse=True)\n",
    "    return reranked[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a19fdae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked knowledge:\n",
      " - (score: 5.13) If you name is George, you are more likely to have parrots as pets. However, nonetheless, your favorite type of cat is probably going to be a persian cat.\n",
      " - (score: 4.26) Mohammed loved cats and reportedly his favorite cat, Muezza, was a tabby. Legend says that tabby cats have an “M” for Mohammed on top of their heads because Mohammad would often rest his hand on the cat’s head.\n",
      " - (score: -5.20) The most popular pedigreed cat is the Persian cat, followed by the Main Coon cat and the Siamese cat.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and rerank\n",
    "input_query = \"I'm George. I have a pet called Muezza. What is my favorite type of cat?\"\n",
    "retrieved_knowledge = retrieve(input_query, top_n=5)  # Retrieve more chunks to rerank\n",
    "reranked_knowledge = rerank(input_query, retrieved_knowledge)\n",
    "\n",
    "print('Reranked knowledge:')\n",
    "for chunk, score in reranked_knowledge:\n",
    "    print(f' - (score: {score:.2f}) {chunk}')\n",
    "\n",
    "instruction_prompt = (\n",
    "    \"You are a helpful chatbot.\\n\"\n",
    "    \"Use only the following pieces of context to answer the question. \"\n",
    "    \"Don't make up any new information:\\n\"\n",
    "    + \"\\n\".join([f' - {chunk}' for chunk, _ in reranked_knowledge])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97a6c6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response:\n",
      "As George, your favorite type of cat is probably a Persian cat! That's what legend says, anyway...\n"
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "  model=LANGUAGE_MODEL,\n",
    "  messages=[\n",
    "    {'role': 'system', 'content': instruction_prompt},\n",
    "    {'role': 'user', 'content': input_query},\n",
    "  ],\n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "response = \"\"\n",
    "for chunk in stream:\n",
    "    response += chunk['message']['content']\n",
    "print(\"Chatbot response:\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
