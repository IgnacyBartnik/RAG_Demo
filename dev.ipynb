{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc359e4b",
   "metadata": {},
   "source": [
    "## Demo of implementing RAG\n",
    "This Jupyter notebook is for an initial implementation of RAG, following the tutorial from [Hugging Face](https://huggingface.co/blog/ngxson/make-your-own-rag)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd542ac",
   "metadata": {},
   "source": [
    "First we begin by loading our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba652e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 entries\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['On average, cats spend 2/3 of every day sleeping. That means a nine-year-old cat has been awake for only three years of its life.\\n',\n",
       " 'Unlike dogs, cats do not have a sweet tooth. Scientists believe this is due to a mutation in a key taste receptor.\\n',\n",
       " 'When a cat chases its prey, it keeps its head level. Dogs and humans bob their heads up and down.\\n',\n",
       " 'The technical term for a cat’s hairball is a “bezoar.”\\n',\n",
       " 'A group of cats is called a “clowder.”\\n']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "with open('cat-facts.txt', 'r') as file:\n",
    "  dataset = file.readlines()\n",
    "  print(f'Loaded {len(dataset)} entries')\n",
    "\n",
    "dataset[0:5]  # Display the first 5 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce06acd",
   "metadata": {},
   "source": [
    "## Implementing vector database\n",
    "\n",
    "We have to convert our plain text to vectors, to be able to use vector similarity search, rather than keyword search (not a viable alternative)\n",
    "<br><br/>\n",
    "To do this, we need to have selected the models we are going to work with. For convenience, we will use Ollama models, as they are free and can be run locally with minimal setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93dbddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f910456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "VECTOR_DB = []\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "  embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "  VECTOR_DB.append((chunk, embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65136e1",
   "metadata": {},
   "source": [
    "We will assume each line in our database to be one chunk. Let us now calculate the embeddings and also add them to the VECTOR_DB list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7274b98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 300 entries from vector_db.txt\n"
     ]
    }
   ],
   "source": [
    "# Let us check if the vectorization has already been done\n",
    "try:\n",
    "  with open('vector_db.txt', 'r') as file:\n",
    "    for line in file:\n",
    "      chunk, embedding_str = line.strip().split('\\t')\n",
    "      embedding = list(map(float, embedding_str.split(',')))\n",
    "      VECTOR_DB.append((chunk, embedding))\n",
    "  print(f'Loaded {len(VECTOR_DB)} entries from vector_db.txt')\n",
    "  vectorized_dataset_loaded = True\n",
    "except FileNotFoundError:\n",
    "  print('vector_db.txt not found, proceeding to vectorize the dataset')\n",
    "  for i, chunk in enumerate(dataset):\n",
    "    add_chunk_to_database(chunk)\n",
    "    print(f'Added chunk {i+1}/{len(dataset)} to the database')\n",
    "  vectorized_dataset_loaded = False\n",
    "    \n",
    "  print(f'Added {len(VECTOR_DB)} chunks to the database')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5350f",
   "metadata": {},
   "source": [
    "Let us save our vectorized database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f12ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors in this list of tuples are <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"The vectors in this list of tuples are {(type(VECTOR_DB[0][1]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d89b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vector database to a txt file\n",
    "if not vectorized_dataset_loaded:\n",
    "  print('Saving the vector database to vector_db.txt')\n",
    "  with open('vector_db.txt', 'w') as file:\n",
    "    for chunk, embedding in VECTOR_DB:\n",
    "      file.write(f\"{chunk.strip()}\\t{','.join(map(str, embedding))}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf58de",
   "metadata": {},
   "source": [
    "## Information Retrival\n",
    "Now we want to implement a function to retrieve information that is closest to the query for the LLM.<br>\n",
    "First we need to make a function that calculates cosine similarity. <br>\n",
    "Lets do it in a way that is fast (even though for this demo, we really don't need to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2dfcab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def cosine_sim_sklearn_vecs(a, b):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors using sklearn.\n",
    "    \"\"\"\n",
    "    a = np.array(a).reshape(1, -1)\n",
    "    b = np.array(b).reshape(1, -1)\n",
    "    return cosine_similarity(a, b)[0, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83869fb9",
   "metadata": {},
   "source": [
    "Now lets implement retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8bf730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "    # Calculate the embedding for the query\n",
    "    query_emb = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "    # Calculate cosine similarity for each chunk in the vector database\n",
    "    similarities = [(chunk, cosine_sim_sklearn_vecs(query_emb, emb)) for chunk, emb in VECTOR_DB]\n",
    "    # Sort the similarities in descending order\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05aae1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input query: what was the largest ever produced cat litter\n",
      "Retrieved knowledge:\n",
      " - (similarity: 0.72) Most cats give birth to a litter of between one and nine kittens. The largest known litter ever produced was 19 kittens, of which 15 survived.\n",
      " - (similarity: 0.72) Most cats give birth to a litter of between one and nine kittens. The largest known litter ever produced was 19 kittens, of which 15 survived.\n",
      " - (similarity: 0.68) A cat called Dusty has the known record for the most kittens. She had more than 420 kittens in her lifetime.\n"
     ]
    }
   ],
   "source": [
    "input_query = input('Ask me a question: ')\n",
    "print(f'Input query: {input_query}')\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "    print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
    "\n",
    "instruction_prompt = (\n",
    "    \"You are a helpful chatbot.\\n\"\n",
    "    \"Use only the following pieces of context to answer the question. \"\n",
    "    \"Don't make up any new information:\\n\"\n",
    "    + \"\\n\".join([f' - {chunk}' for chunk, _ in retrieved_knowledge])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef232408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response:\n",
      "According to the text, there is no information about a \"cat litter\". The text only mentions that cats typically give birth to litters of between one and nine kittens, with some records including a larger litter such as 19 kittens."
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "  model=LANGUAGE_MODEL,\n",
    "  messages=[\n",
    "    {'role': 'system', 'content': instruction_prompt},\n",
    "    {'role': 'user', 'content': input_query},\n",
    "  ],\n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "# print the response from the chatbot in real-time\n",
    "# This might not work in all environments, but should work in most terminals\n",
    "print('Chatbot response:')\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6729bf",
   "metadata": {},
   "source": [
    "We can see that this works, although the prompts could be further tuned for a better response..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
