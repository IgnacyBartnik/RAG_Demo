{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc359e4b",
   "metadata": {},
   "source": [
    "## Demo of implementing RAG\n",
    "This Jupyter notebook is for an initial implementation of RAG, following the tutorial from [Hugging Face](https://huggingface.co/blog/ngxson/make-your-own-rag)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd542ac",
   "metadata": {},
   "source": [
    "First we begin by loading our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba652e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 entries\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['On average, cats spend 2/3 of every day sleeping. That means a nine-year-old cat has been awake for only three years of its life.\\n',\n",
       " 'Unlike dogs, cats do not have a sweet tooth. Scientists believe this is due to a mutation in a key taste receptor.\\n',\n",
       " 'When a cat chases its prey, it keeps its head level. Dogs and humans bob their heads up and down.\\n',\n",
       " 'The technical term for a cat’s hairball is a “bezoar.”\\n',\n",
       " 'A group of cats is called a “clowder.”\\n']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "with open('cat-facts.txt', 'r') as file:\n",
    "  dataset = file.readlines()\n",
    "  print(f'Loaded {len(dataset)} entries')\n",
    "\n",
    "dataset[0:5]  # Display the first 5 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce06acd",
   "metadata": {},
   "source": [
    "## Implementing vector database\n",
    "\n",
    "We have to convert our plain text to vectors, to be able to use vector similarity search, rather than keyword search (not a viable alternative)\n",
    "<br><br/>\n",
    "To do this, we need to have selected the models we are going to work with. For convenience, we will use Ollama models, as they are free and can be run locally with minimal setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93dbddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f910456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "VECTOR_DB = []\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "  embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "  VECTOR_DB.append((chunk, embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65136e1",
   "metadata": {},
   "source": [
    "We will assume each line in our database to be one chunk. Let us now calculate the embeddings and also add them to the VECTOR_DB list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7274b98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 300 entries from vector_db.txt\n"
     ]
    }
   ],
   "source": [
    "# Let us check if the vectorization has already been done\n",
    "try:\n",
    "  with open('vector_db.txt', 'r') as file:\n",
    "    for line in file:\n",
    "      chunk, embedding_str = line.strip().split('\\t')\n",
    "      embedding = list(map(float, embedding_str.split(',')))\n",
    "      VECTOR_DB.append((chunk, embedding))\n",
    "  print(f'Loaded {len(VECTOR_DB)} entries from vector_db.txt')\n",
    "  vectorized_dataset_loaded = True\n",
    "except FileNotFoundError:\n",
    "  print('vector_db.txt not found, proceeding to vectorize the dataset')\n",
    "  for i, chunk in enumerate(dataset):\n",
    "    add_chunk_to_database(chunk)\n",
    "    print(f'Added chunk {i+1}/{len(dataset)} to the database')\n",
    "  vectorized_dataset_loaded = False\n",
    "    \n",
    "  print(f'Added {len(VECTOR_DB)} chunks to the database')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5350f",
   "metadata": {},
   "source": [
    "Let us save our vectorized database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f12ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors in this list of tuples are <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"The vectors in this list of tuples are {(type(VECTOR_DB[0][1]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d89b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vector database to a txt file\n",
    "if not vectorized_dataset_loaded:\n",
    "  print('Saving the vector database to vector_db.txt')\n",
    "  with open('vector_db.txt', 'w') as file:\n",
    "    for chunk, embedding in VECTOR_DB:\n",
    "      file.write(f\"{chunk.strip()}\\t{','.join(map(str, embedding))}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf58de",
   "metadata": {},
   "source": [
    "## Information Retrival\n",
    "Now we want to implement a function to retrieve information that is closest to the query for the LLM.<br>\n",
    "First we need to make a function that calculates cosine similarity. <br>\n",
    "Lets do it in a way that is fast (even though for this demo, we really don't need to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2dfcab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def cosine_sim_sklearn_vecs(a, b):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors using sklearn.\n",
    "    \"\"\"\n",
    "    a = np.array(a).reshape(1, -1)\n",
    "    b = np.array(b).reshape(1, -1)\n",
    "    return cosine_similarity(a, b)[0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d09eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e210ab8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m v1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      2\u001b[0m v2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv2\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# should be ~0.9746\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[36], line 7\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      5\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(a)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(b)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[36], line 7\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      5\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(a)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(b)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "    \u001b[0;31m[... skipping similar frames: cosine_similarity at line 7 (2968 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[36], line 7\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      5\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(a)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(b)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcosine_similarity\u001b[39m(a, b):\n\u001b[0;32m----> 5\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m     b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(b)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cosine_similarity(a, b)[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "v1 = [1, 2, 3]\n",
    "v2 = [4, 5, 6]\n",
    "print(cosine_similarity(v1, v2))  # should be ~0.9746"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83869fb9",
   "metadata": {},
   "source": [
    "Now lets implement retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8bf730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "    # Calculate the embedding for the query\n",
    "    query_emb = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "    # Calculate cosine similarity for each chunk in the vector database\n",
    "    similarities = [(chunk, cosine_similarity(query_emb, emb)) for chunk, emb in VECTOR_DB]\n",
    "    # Sort the similarities in descending order\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05aae1bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-8.73156100e-02 -7.91125900e-02 -4.01601400e-02 -1.92160500e-02\n  5.80570440e-02  2.30034240e-02  2.58869200e-02  6.24126230e-02\n -2.33088680e-02 -5.54876630e-02 -7.04705800e-02 -2.62286530e-02\n -5.67721870e-02  5.56011870e-03  6.52508400e-02  2.27959570e-02\n  2.38364840e-02  3.74786200e-02 -5.18243500e-03 -4.59786540e-02\n  1.25232320e-02 -4.97066160e-02 -7.94574900e-02  2.53541490e-03\n  1.02479760e-02 -4.02277340e-04  2.54537350e-02 -1.42930640e-02\n -6.29645070e-03 -1.90851730e-02  8.49206760e-04 -1.66690440e-02\n  3.20937450e-02 -1.29032400e-02 -2.26669100e-02 -8.04212700e-03\n -1.75521670e-02 -4.40231430e-03  1.29331050e-02 -2.82213070e-02\n -6.83303500e-02  1.22221910e-02  3.89088500e-02  4.91082040e-02\n -6.56129300e-02  2.29920100e-02 -2.19522320e-02 -1.19694950e-02\n -8.97855800e-03  9.62287300e-02 -1.38726330e-02  3.93324230e-02\n -5.11332100e-02 -2.68256420e-02  1.78182600e-02  4.88897600e-02\n  5.36908170e-02 -1.54121870e-02 -3.04700640e-02 -1.86913220e-02\n  3.55234500e-02  4.68432100e-03  2.67336000e-02 -2.85777200e-03\n -3.36041800e-02 -7.06452400e-04  4.35268800e-02  1.66627880e-02\n -2.36808840e-02 -4.06086930e-02  4.49341600e-03  4.18684330e-02\n  1.61616280e-02 -2.54339330e-02 -1.37145575e-02 -7.20840900e-03\n -2.39583720e-02 -6.24731030e-02  5.30952330e-02  3.97342850e-02\n  2.31808330e-02  1.73396620e-02  5.95907980e-02  4.71590800e-02\n  8.96248100e-05  1.53619050e-02  1.20906970e-02 -1.07714990e-02\n -1.63133850e-02  5.45357060e-02 -9.41237800e-03 -1.91355780e-02\n  2.75700870e-02  7.05465200e-02  2.22682450e-02 -4.62756940e-02\n  1.02647490e-02 -2.72721910e-03 -1.29517580e-03  6.14152940e-03\n -4.44627140e-02  4.02036160e-02  9.80356400e-03 -3.39649470e-02\n -7.45756100e-02  1.34406260e-02 -3.31645720e-02 -2.17045060e-02\n  2.47405750e-02  3.75416100e-02 -3.09970290e-02 -3.48584760e-02\n  2.79963780e-02  2.97953670e-02 -2.76834160e-02  5.54620000e-02\n  2.16124240e-02  7.92200400e-03  2.90352730e-03 -4.91054170e-03\n  6.05890450e-02  3.98060500e-02  2.45259900e-02  6.21534770e-02\n  2.88674920e-02  2.80874190e-02 -1.42978950e-02  1.73074160e-03\n  1.45334710e-02 -2.72064920e-02  1.58636420e-02  9.63381400e-03\n -2.62686160e-02 -1.21304690e-02 -1.92579720e-02 -3.17700070e-03\n -3.48698420e-02 -1.58197530e-02  2.54617740e-02  1.79697760e-02\n -1.21143060e-02  1.59166770e-02 -2.55671010e-02 -2.63327900e-02\n  2.54322050e-03 -4.09155970e-02 -3.18658720e-02 -4.12685800e-02\n  5.65774500e-03  2.16722620e-02 -2.69711180e-03 -1.67402200e-02\n -2.98515320e-02 -2.83416200e-02 -1.21132310e-02  5.28232160e-02\n -1.20679110e-02  7.68635600e-03 -4.06774250e-02 -4.15222270e-02\n  4.86178140e-02  4.44864650e-04  4.22681270e-02  2.70839350e-02\n -3.70837400e-02  4.02526150e-02  1.57396630e-02  3.26420030e-02\n -6.62649650e-02  5.63310050e-02 -3.01537590e-02  2.47274300e-02\n  7.89365000e-02  2.96060630e-02 -1.36453565e-02  2.47065310e-02\n  8.81516800e-02 -1.93812340e-02 -8.32861800e-03  5.08224600e-02\n -4.88216880e-02  5.73431700e-02 -2.33393600e-02 -1.23403575e-02\n  2.65627220e-02  2.84885660e-03  3.88818420e-02 -3.76901960e-02\n  1.72488690e-02 -4.58767970e-02  1.99716740e-02 -1.61405060e-02\n -1.13896070e-02  2.53033500e-03  2.21328800e-02 -2.63261890e-02\n -1.42383530e-02 -2.45754140e-02  4.30987180e-02  4.63467240e-02\n -1.21085430e-02 -2.09258750e-03 -2.87395720e-02 -2.50899500e-02\n -3.69241100e-02  1.08312230e-02 -1.09283630e-02 -5.76842000e-03\n  4.57425860e-02  5.64326640e-02  8.99709200e-04 -2.06445530e-02\n -4.03674240e-02  1.04594800e-02 -6.94932930e-03 -8.97129360e-04\n -4.99816000e-03 -6.52069300e-02 -6.96048400e-04 -2.69344400e-02\n -3.32698260e-02 -1.78742690e-03 -7.02374900e-02 -1.60482880e-02\n -1.72931890e-02  7.20656300e-02  2.91700360e-02 -8.17045600e-02\n  6.07163300e-02  5.24935500e-02 -1.27198870e-02 -2.70818320e-02\n  3.05970090e-02  3.51696500e-02 -1.07194430e-03 -4.60987300e-02\n  4.53902830e-03 -3.72414850e-02  2.94892120e-02 -4.64794200e-02\n -4.09858380e-02  7.87178300e-02  4.26481630e-02  6.27571940e-02\n -1.48373535e-02 -4.52095640e-02  6.60514700e-02 -1.93451460e-02\n  2.38854140e-02 -2.37997900e-02  2.81918140e-03  3.10755900e-03\n -2.11907290e-02 -8.69527500e-03 -2.40445160e-02 -3.22569570e-02\n -3.63208800e-02  9.57768400e-03 -1.62079940e-04  2.32091960e-02\n -1.09304255e-02  3.98505030e-02 -1.57203320e-02  4.04856020e-02\n -1.83238020e-03 -9.60934200e-02 -4.85122200e-02  1.46212890e-02\n  1.36492600e-02 -3.59217340e-02  1.28407860e-02 -6.21505400e-03\n -3.34771300e-03 -3.27271150e-02 -1.60537270e-05 -4.56285240e-03\n  1.61634600e-03  1.36047480e-02  1.96334480e-02 -7.07581200e-02\n -1.16789470e-02  6.31035100e-02 -1.22025640e-02 -5.20493760e-02\n -6.01872900e-03 -3.46414800e-02 -6.19757300e-03 -5.91990540e-02\n -6.71147400e-03  1.21294870e-02  4.09565160e-02 -9.21388600e-03\n -1.27529510e-02 -4.72936300e-02 -6.18560800e-02  3.35302930e-03\n -3.10658330e-04  7.40737240e-02 -1.33338320e-02 -4.34933980e-02\n  2.48385690e-02  4.73900370e-03 -1.65361860e-02  2.11092940e-02\n  4.01785700e-02 -7.47094630e-04  3.07627700e-03  6.65393150e-03\n -2.50134380e-01 -1.54324840e-02  2.92657460e-02 -1.73214920e-02\n  4.31448300e-02 -2.62016190e-02  1.80549120e-02 -5.54316530e-02\n  1.15482670e-02  1.22501620e-02 -8.34871000e-03 -6.14152150e-03\n  1.96268990e-02  3.72849000e-02 -1.13898200e-02 -2.63908110e-02\n -5.21883200e-02 -4.47687830e-03 -1.98762860e-02  2.86528100e-02\n -1.80709370e-03 -3.15510400e-02 -4.59071030e-02  3.61659040e-02\n -6.56428560e-02  5.24330770e-02 -5.78888950e-02 -4.75970840e-02\n -2.17375440e-02  7.52730000e-03 -5.07487060e-02 -7.29434370e-04\n -4.04892600e-02  1.31305240e-02  2.54267640e-02  7.19107040e-03\n -1.84348320e-02 -6.09220970e-02 -4.62950100e-02  5.06342460e-02\n -7.41606400e-05 -5.70202130e-02 -3.74223500e-02 -9.12538100e-03\n -1.63561500e-02 -2.02700890e-02  1.50676700e-02 -3.99945900e-02\n  2.41138000e-02  4.20206750e-02 -4.66041600e-02  4.96821800e-03\n -4.01277770e-02 -1.36603240e-02  3.29643970e-02 -2.74405240e-02\n -3.14166800e-02  1.39302250e-03 -2.82924970e-03 -8.82276300e-03\n -2.19173470e-02 -2.38845700e-02 -1.22571350e-02 -1.30690610e-02\n -3.17775500e-02 -1.76055360e-02  3.32193340e-02 -1.97351100e-02\n  8.36201160e-02 -4.02520150e-02  4.73529600e-02 -7.11241660e-02\n -6.72282950e-02 -8.09339660e-02 -3.02848520e-02  1.29698150e-02\n -3.36323120e-02  9.26282350e-03 -2.03374160e-02  6.06810900e-02\n -7.93677100e-02  8.68885400e-03 -3.26911470e-02  4.80562300e-03\n -3.05896510e-02 -9.38348100e-02  2.06999240e-02  3.72823400e-02\n -1.79507150e-02 -4.28675450e-02  1.06690000e-02 -4.04498730e-02\n -3.06409170e-02  5.08525870e-03  6.87969900e-02  2.91599670e-02\n -2.04085780e-02  2.06921360e-02 -1.56065340e-02  1.61404460e-02\n  3.62376700e-02 -4.13869000e-02  3.04226820e-02 -2.21536400e-02\n -2.97853050e-02 -2.18325670e-02  2.18714460e-02  2.60486860e-02\n -9.87599000e-04 -4.58369630e-02  4.31209800e-02 -3.59833280e-02\n  5.77215070e-02 -4.34725960e-02  2.62499770e-02  1.79612170e-02\n  5.65180670e-02  3.50030700e-02  3.76017060e-02  4.66713980e-02\n  7.08510630e-03  4.12547100e-02 -6.25559540e-02  1.64968070e-02\n -6.00154100e-02 -1.89909670e-02 -1.46621390e-02 -2.12191030e-02\n  2.42369440e-02 -5.28139920e-02 -3.29311860e-02  2.32085530e-02\n  3.39269400e-02  2.15209460e-02  7.50640500e-02 -4.12307260e-03\n -8.22289300e-03 -9.97316700e-02 -8.47745900e-03 -3.38871930e-02\n -4.16590240e-02  8.97445800e-02  2.71599600e-04  4.78802440e-02\n -3.28132170e-02  7.08814430e-03  5.02039750e-02 -1.67337380e-02\n  5.93066030e-03  5.10513200e-02  1.69826580e-02 -2.72421610e-02\n  3.41311300e-02 -2.43501770e-02  1.50088130e-02 -4.49257230e-02\n -5.75963270e-03  2.03043520e-02 -4.55775070e-02 -4.70134360e-02\n -8.23600200e-03 -6.36305500e-02  3.46953870e-02 -1.10692470e-02\n -1.25722190e-03  1.92142240e-02  1.70908640e-02  2.95193550e-02\n  1.56027830e-02  5.74395360e-02  3.71476300e-02 -1.02851000e-02\n  2.53033430e-02  1.93776490e-02  3.57547070e-02  1.02993810e-02\n -7.88863200e-03  1.10358900e-02 -1.59742010e-02  9.68069600e-03\n -7.01863200e-03 -4.74151970e-03 -2.54288950e-02 -5.38176800e-03\n  3.87748330e-02  1.86550260e-02 -2.91735870e-02 -2.89030220e-02\n  4.36137060e-02  4.35860700e-02 -2.47045100e-02  7.75783060e-02\n -4.84037300e-02 -1.47565010e-02 -5.24382370e-02 -3.02614460e-02\n  1.03800220e-02  1.41530760e-02  1.93515070e-02  3.14436700e-02\n  2.86365540e-02  8.94833000e-03  3.29295580e-04  2.83647900e-02\n -1.67806260e-02 -4.27854320e-02  4.24226670e-02  1.32342420e-02\n -1.57620980e-02  5.99792900e-03 -3.44890020e-02 -5.75438100e-02\n  1.64353860e-03  4.07295640e-02  4.06123920e-02  4.48288320e-02\n -4.17945120e-02  2.09081450e-02  1.28569790e-02 -5.03110070e-02\n -6.49758900e-02 -9.63168800e-03 -3.32273470e-02  7.55096040e-05\n  9.87924000e-03  4.14957740e-02  1.73887550e-02 -1.31812140e-02\n  7.68631550e-02  9.82459800e-04  3.71592680e-02 -2.31895580e-02\n  1.99853180e-02  5.16490660e-04  3.02017330e-02  2.20586580e-02\n  4.89406850e-02 -3.70659340e-02 -4.09197200e-02  2.81972180e-02\n  2.11924600e-02 -3.52605060e-02 -2.64374390e-02 -4.65784780e-02\n  1.41165070e-03  2.38227400e-02 -2.67580570e-03 -4.34409270e-02\n  2.02497650e-02  8.15710400e-03  3.80934370e-02  8.61860250e-03\n -3.66528220e-02  3.02821000e-02  6.14796770e-02 -1.81897600e-02\n -5.40461470e-02  3.42643000e-02 -2.84823400e-02  5.09240330e-02\n  4.25264800e-02  3.22477970e-02 -3.85215530e-03  4.19208600e-02\n  1.11059435e-02  3.19924900e-04 -1.10318820e-02 -5.04352900e-02\n  3.20512240e-02  1.47554360e-02  5.92616850e-03 -6.07502870e-02\n  2.97430420e-02  2.37140390e-03  3.27775140e-02 -1.57431940e-02\n  5.89333100e-02  7.19285200e-03 -1.76082770e-02 -6.34428600e-02\n -1.09181410e-03  4.61751070e-02 -1.07356550e-02  3.42627800e-02\n -1.82891490e-02 -1.47278560e-02  1.17136520e-02 -2.71184910e-02\n -3.25167500e-02 -3.43855870e-03  1.10154480e-02  5.70086280e-02\n -3.37325740e-03  1.75695380e-02  1.04062590e-02  3.47390060e-02\n  5.13364370e-02  4.98380250e-02  3.02580730e-02 -4.77849320e-02\n -4.51554950e-02  9.63569900e-02 -6.97593300e-03 -7.57401100e-03\n -6.98744830e-03 -3.30806820e-02 -1.91925280e-02 -6.43196250e-02\n -3.49131520e-02  9.72128000e-03  1.21750770e-02 -6.89609100e-02\n  1.26629360e-02  1.59219400e-02 -8.13210400e-04  2.09771630e-03\n  1.95637330e-02 -5.25104370e-02  4.81328960e-02 -2.29685420e-02\n  2.95234290e-02 -1.15696340e-02  2.43113840e-02  4.09034200e-02\n  2.90235150e-02  3.73067630e-02 -4.40889560e-02  3.59983800e-02\n  5.93317150e-02  5.12089300e-02  2.69227640e-02  3.26254960e-02\n -3.06578780e-02  2.92046520e-02 -2.64359200e-02 -5.77082400e-02\n  6.80101570e-03 -6.88694400e-03  2.99576260e-02 -5.15707300e-03\n  3.60804870e-02 -1.10612580e-02 -9.01859200e-03  4.05551700e-02\n  7.66858550e-03  3.76094100e-02  1.92061850e-02 -7.25835200e-02\n  1.22388890e-02  2.30834000e-02 -8.61646700e-03 -2.11331580e-02\n -3.34319200e-02  5.46508930e-03  1.69832930e-02  2.59569980e-02\n -2.06492080e-02 -4.59713000e-02 -5.47304750e-03 -2.23314180e-03\n  2.68061180e-02  8.27927140e-02  1.86546240e-02 -1.72485450e-02\n -1.86248160e-02  5.49130070e-02  3.13944600e-02  1.42842070e-02\n  1.07123670e-02  4.00511900e-02  2.10431480e-02  3.56283500e-04\n -1.43621120e-02  5.75350450e-02  4.18171440e-02  1.52280160e-02\n  1.84030700e-02 -2.12816960e-03 -4.96719000e-02  2.76079750e-03\n -5.29532130e-03  1.33036050e-02 -1.72617710e-02 -2.38461480e-02\n  9.77228300e-03 -1.25736315e-02 -2.11037770e-02 -5.33249500e-04\n -3.53625940e-02 -2.57179600e-02 -4.10598630e-02  5.25485770e-03\n -7.06483470e-03  5.24371750e-02 -3.10875660e-02 -3.31128240e-02\n  4.51481830e-02  3.32627400e-02 -9.25085200e-02 -3.11126000e-03\n  9.75731100e-02  9.79211100e-03 -5.90692970e-03 -1.31111825e-02\n  1.26140130e-04  4.04992440e-02 -1.89267620e-02 -1.45965135e-02\n -6.43366650e-02  5.32982650e-02  1.91938410e-02  1.95669120e-02\n -1.10431750e-02 -3.00997050e-02  1.75340850e-02 -9.87306800e-03\n  3.85870640e-02 -3.64179900e-02  2.22937610e-02 -1.69109180e-02\n  9.91418900e-03 -1.32685270e-02  6.26270430e-03  3.25245220e-02\n -3.50674880e-02 -9.82728600e-04  5.18955140e-02 -2.09851700e-02\n -4.73425130e-02  9.07401000e-02 -1.19450930e-02  4.96802740e-02\n -2.33184660e-02 -3.19452900e-02 -4.98313120e-02 -1.04046800e-01\n  5.23928370e-02  3.45330540e-02  1.26748830e-02 -5.39423080e-02\n  2.21197500e-02  7.21547150e-03 -3.74118420e-02  3.03993100e-03\n  7.29870400e-03 -5.55075560e-02  7.33643720e-03 -2.33283060e-02\n -2.99838450e-02 -2.54164000e-02 -5.43985140e-02  8.64263300e-03\n -3.67243700e-02  1.44003040e-02 -1.33153100e-02 -9.08451300e-03\n -3.60475600e-03  4.91245720e-02  3.65207830e-02  8.46474100e-03].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsk me a question: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m retrieved_knowledge \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieved knowledge:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk, similarity \u001b[38;5;129;01min\u001b[39;00m retrieved_knowledge:\n",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(query, top_n)\u001b[0m\n\u001b[1;32m      3\u001b[0m query_emb \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39membed(model\u001b[38;5;241m=\u001b[39mEMBEDDING_MODEL, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mquery)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarity for each chunk in the vector database\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m similarities \u001b[38;5;241m=\u001b[39m [(chunk, cosine_similarity(query_emb, emb)) \u001b[38;5;28;01mfor\u001b[39;00m chunk, emb \u001b[38;5;129;01min\u001b[39;00m VECTOR_DB]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Sort the similarities in descending order\u001b[39;00m\n\u001b[1;32m      7\u001b[0m similarities\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m query_emb \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39membed(model\u001b[38;5;241m=\u001b[39mEMBEDDING_MODEL, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mquery)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarity for each chunk in the vector database\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m similarities \u001b[38;5;241m=\u001b[39m [(chunk, \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m chunk, emb \u001b[38;5;129;01min\u001b[39;00m VECTOR_DB]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Sort the similarities in descending order\u001b[39;00m\n\u001b[1;32m      7\u001b[0m similarities\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/RAG_Demo/rag/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    228\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/RAG_Demo/rag/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1728\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1676\u001b[0m     {\n\u001b[1;32m   1677\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m )\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcosine_similarity\u001b[39m(X, Y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1685\u001b[0m \n\u001b[1;32m   1686\u001b[0m \u001b[38;5;124;03m    Cosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[38;5;124;03m           [0.577, 0.816]])\u001b[39;00m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1728\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m     X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m~/Documents/RAG_Demo/rag/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:200\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[1;32m    190\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    191\u001b[0m         X,\n\u001b[1;32m    192\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    210\u001b[0m         Y,\n\u001b[1;32m    211\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[0;32m~/Documents/RAG_Demo/rag/lib/python3.10/site-packages/sklearn/utils/validation.py:1091\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1085\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1086\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1087\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1088\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m             )\n\u001b[0;32m-> 1091\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1095\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1096\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1097\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-8.73156100e-02 -7.91125900e-02 -4.01601400e-02 -1.92160500e-02\n  5.80570440e-02  2.30034240e-02  2.58869200e-02  6.24126230e-02\n -2.33088680e-02 -5.54876630e-02 -7.04705800e-02 -2.62286530e-02\n -5.67721870e-02  5.56011870e-03  6.52508400e-02  2.27959570e-02\n  2.38364840e-02  3.74786200e-02 -5.18243500e-03 -4.59786540e-02\n  1.25232320e-02 -4.97066160e-02 -7.94574900e-02  2.53541490e-03\n  1.02479760e-02 -4.02277340e-04  2.54537350e-02 -1.42930640e-02\n -6.29645070e-03 -1.90851730e-02  8.49206760e-04 -1.66690440e-02\n  3.20937450e-02 -1.29032400e-02 -2.26669100e-02 -8.04212700e-03\n -1.75521670e-02 -4.40231430e-03  1.29331050e-02 -2.82213070e-02\n -6.83303500e-02  1.22221910e-02  3.89088500e-02  4.91082040e-02\n -6.56129300e-02  2.29920100e-02 -2.19522320e-02 -1.19694950e-02\n -8.97855800e-03  9.62287300e-02 -1.38726330e-02  3.93324230e-02\n -5.11332100e-02 -2.68256420e-02  1.78182600e-02  4.88897600e-02\n  5.36908170e-02 -1.54121870e-02 -3.04700640e-02 -1.86913220e-02\n  3.55234500e-02  4.68432100e-03  2.67336000e-02 -2.85777200e-03\n -3.36041800e-02 -7.06452400e-04  4.35268800e-02  1.66627880e-02\n -2.36808840e-02 -4.06086930e-02  4.49341600e-03  4.18684330e-02\n  1.61616280e-02 -2.54339330e-02 -1.37145575e-02 -7.20840900e-03\n -2.39583720e-02 -6.24731030e-02  5.30952330e-02  3.97342850e-02\n  2.31808330e-02  1.73396620e-02  5.95907980e-02  4.71590800e-02\n  8.96248100e-05  1.53619050e-02  1.20906970e-02 -1.07714990e-02\n -1.63133850e-02  5.45357060e-02 -9.41237800e-03 -1.91355780e-02\n  2.75700870e-02  7.05465200e-02  2.22682450e-02 -4.62756940e-02\n  1.02647490e-02 -2.72721910e-03 -1.29517580e-03  6.14152940e-03\n -4.44627140e-02  4.02036160e-02  9.80356400e-03 -3.39649470e-02\n -7.45756100e-02  1.34406260e-02 -3.31645720e-02 -2.17045060e-02\n  2.47405750e-02  3.75416100e-02 -3.09970290e-02 -3.48584760e-02\n  2.79963780e-02  2.97953670e-02 -2.76834160e-02  5.54620000e-02\n  2.16124240e-02  7.92200400e-03  2.90352730e-03 -4.91054170e-03\n  6.05890450e-02  3.98060500e-02  2.45259900e-02  6.21534770e-02\n  2.88674920e-02  2.80874190e-02 -1.42978950e-02  1.73074160e-03\n  1.45334710e-02 -2.72064920e-02  1.58636420e-02  9.63381400e-03\n -2.62686160e-02 -1.21304690e-02 -1.92579720e-02 -3.17700070e-03\n -3.48698420e-02 -1.58197530e-02  2.54617740e-02  1.79697760e-02\n -1.21143060e-02  1.59166770e-02 -2.55671010e-02 -2.63327900e-02\n  2.54322050e-03 -4.09155970e-02 -3.18658720e-02 -4.12685800e-02\n  5.65774500e-03  2.16722620e-02 -2.69711180e-03 -1.67402200e-02\n -2.98515320e-02 -2.83416200e-02 -1.21132310e-02  5.28232160e-02\n -1.20679110e-02  7.68635600e-03 -4.06774250e-02 -4.15222270e-02\n  4.86178140e-02  4.44864650e-04  4.22681270e-02  2.70839350e-02\n -3.70837400e-02  4.02526150e-02  1.57396630e-02  3.26420030e-02\n -6.62649650e-02  5.63310050e-02 -3.01537590e-02  2.47274300e-02\n  7.89365000e-02  2.96060630e-02 -1.36453565e-02  2.47065310e-02\n  8.81516800e-02 -1.93812340e-02 -8.32861800e-03  5.08224600e-02\n -4.88216880e-02  5.73431700e-02 -2.33393600e-02 -1.23403575e-02\n  2.65627220e-02  2.84885660e-03  3.88818420e-02 -3.76901960e-02\n  1.72488690e-02 -4.58767970e-02  1.99716740e-02 -1.61405060e-02\n -1.13896070e-02  2.53033500e-03  2.21328800e-02 -2.63261890e-02\n -1.42383530e-02 -2.45754140e-02  4.30987180e-02  4.63467240e-02\n -1.21085430e-02 -2.09258750e-03 -2.87395720e-02 -2.50899500e-02\n -3.69241100e-02  1.08312230e-02 -1.09283630e-02 -5.76842000e-03\n  4.57425860e-02  5.64326640e-02  8.99709200e-04 -2.06445530e-02\n -4.03674240e-02  1.04594800e-02 -6.94932930e-03 -8.97129360e-04\n -4.99816000e-03 -6.52069300e-02 -6.96048400e-04 -2.69344400e-02\n -3.32698260e-02 -1.78742690e-03 -7.02374900e-02 -1.60482880e-02\n -1.72931890e-02  7.20656300e-02  2.91700360e-02 -8.17045600e-02\n  6.07163300e-02  5.24935500e-02 -1.27198870e-02 -2.70818320e-02\n  3.05970090e-02  3.51696500e-02 -1.07194430e-03 -4.60987300e-02\n  4.53902830e-03 -3.72414850e-02  2.94892120e-02 -4.64794200e-02\n -4.09858380e-02  7.87178300e-02  4.26481630e-02  6.27571940e-02\n -1.48373535e-02 -4.52095640e-02  6.60514700e-02 -1.93451460e-02\n  2.38854140e-02 -2.37997900e-02  2.81918140e-03  3.10755900e-03\n -2.11907290e-02 -8.69527500e-03 -2.40445160e-02 -3.22569570e-02\n -3.63208800e-02  9.57768400e-03 -1.62079940e-04  2.32091960e-02\n -1.09304255e-02  3.98505030e-02 -1.57203320e-02  4.04856020e-02\n -1.83238020e-03 -9.60934200e-02 -4.85122200e-02  1.46212890e-02\n  1.36492600e-02 -3.59217340e-02  1.28407860e-02 -6.21505400e-03\n -3.34771300e-03 -3.27271150e-02 -1.60537270e-05 -4.56285240e-03\n  1.61634600e-03  1.36047480e-02  1.96334480e-02 -7.07581200e-02\n -1.16789470e-02  6.31035100e-02 -1.22025640e-02 -5.20493760e-02\n -6.01872900e-03 -3.46414800e-02 -6.19757300e-03 -5.91990540e-02\n -6.71147400e-03  1.21294870e-02  4.09565160e-02 -9.21388600e-03\n -1.27529510e-02 -4.72936300e-02 -6.18560800e-02  3.35302930e-03\n -3.10658330e-04  7.40737240e-02 -1.33338320e-02 -4.34933980e-02\n  2.48385690e-02  4.73900370e-03 -1.65361860e-02  2.11092940e-02\n  4.01785700e-02 -7.47094630e-04  3.07627700e-03  6.65393150e-03\n -2.50134380e-01 -1.54324840e-02  2.92657460e-02 -1.73214920e-02\n  4.31448300e-02 -2.62016190e-02  1.80549120e-02 -5.54316530e-02\n  1.15482670e-02  1.22501620e-02 -8.34871000e-03 -6.14152150e-03\n  1.96268990e-02  3.72849000e-02 -1.13898200e-02 -2.63908110e-02\n -5.21883200e-02 -4.47687830e-03 -1.98762860e-02  2.86528100e-02\n -1.80709370e-03 -3.15510400e-02 -4.59071030e-02  3.61659040e-02\n -6.56428560e-02  5.24330770e-02 -5.78888950e-02 -4.75970840e-02\n -2.17375440e-02  7.52730000e-03 -5.07487060e-02 -7.29434370e-04\n -4.04892600e-02  1.31305240e-02  2.54267640e-02  7.19107040e-03\n -1.84348320e-02 -6.09220970e-02 -4.62950100e-02  5.06342460e-02\n -7.41606400e-05 -5.70202130e-02 -3.74223500e-02 -9.12538100e-03\n -1.63561500e-02 -2.02700890e-02  1.50676700e-02 -3.99945900e-02\n  2.41138000e-02  4.20206750e-02 -4.66041600e-02  4.96821800e-03\n -4.01277770e-02 -1.36603240e-02  3.29643970e-02 -2.74405240e-02\n -3.14166800e-02  1.39302250e-03 -2.82924970e-03 -8.82276300e-03\n -2.19173470e-02 -2.38845700e-02 -1.22571350e-02 -1.30690610e-02\n -3.17775500e-02 -1.76055360e-02  3.32193340e-02 -1.97351100e-02\n  8.36201160e-02 -4.02520150e-02  4.73529600e-02 -7.11241660e-02\n -6.72282950e-02 -8.09339660e-02 -3.02848520e-02  1.29698150e-02\n -3.36323120e-02  9.26282350e-03 -2.03374160e-02  6.06810900e-02\n -7.93677100e-02  8.68885400e-03 -3.26911470e-02  4.80562300e-03\n -3.05896510e-02 -9.38348100e-02  2.06999240e-02  3.72823400e-02\n -1.79507150e-02 -4.28675450e-02  1.06690000e-02 -4.04498730e-02\n -3.06409170e-02  5.08525870e-03  6.87969900e-02  2.91599670e-02\n -2.04085780e-02  2.06921360e-02 -1.56065340e-02  1.61404460e-02\n  3.62376700e-02 -4.13869000e-02  3.04226820e-02 -2.21536400e-02\n -2.97853050e-02 -2.18325670e-02  2.18714460e-02  2.60486860e-02\n -9.87599000e-04 -4.58369630e-02  4.31209800e-02 -3.59833280e-02\n  5.77215070e-02 -4.34725960e-02  2.62499770e-02  1.79612170e-02\n  5.65180670e-02  3.50030700e-02  3.76017060e-02  4.66713980e-02\n  7.08510630e-03  4.12547100e-02 -6.25559540e-02  1.64968070e-02\n -6.00154100e-02 -1.89909670e-02 -1.46621390e-02 -2.12191030e-02\n  2.42369440e-02 -5.28139920e-02 -3.29311860e-02  2.32085530e-02\n  3.39269400e-02  2.15209460e-02  7.50640500e-02 -4.12307260e-03\n -8.22289300e-03 -9.97316700e-02 -8.47745900e-03 -3.38871930e-02\n -4.16590240e-02  8.97445800e-02  2.71599600e-04  4.78802440e-02\n -3.28132170e-02  7.08814430e-03  5.02039750e-02 -1.67337380e-02\n  5.93066030e-03  5.10513200e-02  1.69826580e-02 -2.72421610e-02\n  3.41311300e-02 -2.43501770e-02  1.50088130e-02 -4.49257230e-02\n -5.75963270e-03  2.03043520e-02 -4.55775070e-02 -4.70134360e-02\n -8.23600200e-03 -6.36305500e-02  3.46953870e-02 -1.10692470e-02\n -1.25722190e-03  1.92142240e-02  1.70908640e-02  2.95193550e-02\n  1.56027830e-02  5.74395360e-02  3.71476300e-02 -1.02851000e-02\n  2.53033430e-02  1.93776490e-02  3.57547070e-02  1.02993810e-02\n -7.88863200e-03  1.10358900e-02 -1.59742010e-02  9.68069600e-03\n -7.01863200e-03 -4.74151970e-03 -2.54288950e-02 -5.38176800e-03\n  3.87748330e-02  1.86550260e-02 -2.91735870e-02 -2.89030220e-02\n  4.36137060e-02  4.35860700e-02 -2.47045100e-02  7.75783060e-02\n -4.84037300e-02 -1.47565010e-02 -5.24382370e-02 -3.02614460e-02\n  1.03800220e-02  1.41530760e-02  1.93515070e-02  3.14436700e-02\n  2.86365540e-02  8.94833000e-03  3.29295580e-04  2.83647900e-02\n -1.67806260e-02 -4.27854320e-02  4.24226670e-02  1.32342420e-02\n -1.57620980e-02  5.99792900e-03 -3.44890020e-02 -5.75438100e-02\n  1.64353860e-03  4.07295640e-02  4.06123920e-02  4.48288320e-02\n -4.17945120e-02  2.09081450e-02  1.28569790e-02 -5.03110070e-02\n -6.49758900e-02 -9.63168800e-03 -3.32273470e-02  7.55096040e-05\n  9.87924000e-03  4.14957740e-02  1.73887550e-02 -1.31812140e-02\n  7.68631550e-02  9.82459800e-04  3.71592680e-02 -2.31895580e-02\n  1.99853180e-02  5.16490660e-04  3.02017330e-02  2.20586580e-02\n  4.89406850e-02 -3.70659340e-02 -4.09197200e-02  2.81972180e-02\n  2.11924600e-02 -3.52605060e-02 -2.64374390e-02 -4.65784780e-02\n  1.41165070e-03  2.38227400e-02 -2.67580570e-03 -4.34409270e-02\n  2.02497650e-02  8.15710400e-03  3.80934370e-02  8.61860250e-03\n -3.66528220e-02  3.02821000e-02  6.14796770e-02 -1.81897600e-02\n -5.40461470e-02  3.42643000e-02 -2.84823400e-02  5.09240330e-02\n  4.25264800e-02  3.22477970e-02 -3.85215530e-03  4.19208600e-02\n  1.11059435e-02  3.19924900e-04 -1.10318820e-02 -5.04352900e-02\n  3.20512240e-02  1.47554360e-02  5.92616850e-03 -6.07502870e-02\n  2.97430420e-02  2.37140390e-03  3.27775140e-02 -1.57431940e-02\n  5.89333100e-02  7.19285200e-03 -1.76082770e-02 -6.34428600e-02\n -1.09181410e-03  4.61751070e-02 -1.07356550e-02  3.42627800e-02\n -1.82891490e-02 -1.47278560e-02  1.17136520e-02 -2.71184910e-02\n -3.25167500e-02 -3.43855870e-03  1.10154480e-02  5.70086280e-02\n -3.37325740e-03  1.75695380e-02  1.04062590e-02  3.47390060e-02\n  5.13364370e-02  4.98380250e-02  3.02580730e-02 -4.77849320e-02\n -4.51554950e-02  9.63569900e-02 -6.97593300e-03 -7.57401100e-03\n -6.98744830e-03 -3.30806820e-02 -1.91925280e-02 -6.43196250e-02\n -3.49131520e-02  9.72128000e-03  1.21750770e-02 -6.89609100e-02\n  1.26629360e-02  1.59219400e-02 -8.13210400e-04  2.09771630e-03\n  1.95637330e-02 -5.25104370e-02  4.81328960e-02 -2.29685420e-02\n  2.95234290e-02 -1.15696340e-02  2.43113840e-02  4.09034200e-02\n  2.90235150e-02  3.73067630e-02 -4.40889560e-02  3.59983800e-02\n  5.93317150e-02  5.12089300e-02  2.69227640e-02  3.26254960e-02\n -3.06578780e-02  2.92046520e-02 -2.64359200e-02 -5.77082400e-02\n  6.80101570e-03 -6.88694400e-03  2.99576260e-02 -5.15707300e-03\n  3.60804870e-02 -1.10612580e-02 -9.01859200e-03  4.05551700e-02\n  7.66858550e-03  3.76094100e-02  1.92061850e-02 -7.25835200e-02\n  1.22388890e-02  2.30834000e-02 -8.61646700e-03 -2.11331580e-02\n -3.34319200e-02  5.46508930e-03  1.69832930e-02  2.59569980e-02\n -2.06492080e-02 -4.59713000e-02 -5.47304750e-03 -2.23314180e-03\n  2.68061180e-02  8.27927140e-02  1.86546240e-02 -1.72485450e-02\n -1.86248160e-02  5.49130070e-02  3.13944600e-02  1.42842070e-02\n  1.07123670e-02  4.00511900e-02  2.10431480e-02  3.56283500e-04\n -1.43621120e-02  5.75350450e-02  4.18171440e-02  1.52280160e-02\n  1.84030700e-02 -2.12816960e-03 -4.96719000e-02  2.76079750e-03\n -5.29532130e-03  1.33036050e-02 -1.72617710e-02 -2.38461480e-02\n  9.77228300e-03 -1.25736315e-02 -2.11037770e-02 -5.33249500e-04\n -3.53625940e-02 -2.57179600e-02 -4.10598630e-02  5.25485770e-03\n -7.06483470e-03  5.24371750e-02 -3.10875660e-02 -3.31128240e-02\n  4.51481830e-02  3.32627400e-02 -9.25085200e-02 -3.11126000e-03\n  9.75731100e-02  9.79211100e-03 -5.90692970e-03 -1.31111825e-02\n  1.26140130e-04  4.04992440e-02 -1.89267620e-02 -1.45965135e-02\n -6.43366650e-02  5.32982650e-02  1.91938410e-02  1.95669120e-02\n -1.10431750e-02 -3.00997050e-02  1.75340850e-02 -9.87306800e-03\n  3.85870640e-02 -3.64179900e-02  2.22937610e-02 -1.69109180e-02\n  9.91418900e-03 -1.32685270e-02  6.26270430e-03  3.25245220e-02\n -3.50674880e-02 -9.82728600e-04  5.18955140e-02 -2.09851700e-02\n -4.73425130e-02  9.07401000e-02 -1.19450930e-02  4.96802740e-02\n -2.33184660e-02 -3.19452900e-02 -4.98313120e-02 -1.04046800e-01\n  5.23928370e-02  3.45330540e-02  1.26748830e-02 -5.39423080e-02\n  2.21197500e-02  7.21547150e-03 -3.74118420e-02  3.03993100e-03\n  7.29870400e-03 -5.55075560e-02  7.33643720e-03 -2.33283060e-02\n -2.99838450e-02 -2.54164000e-02 -5.43985140e-02  8.64263300e-03\n -3.67243700e-02  1.44003040e-02 -1.33153100e-02 -9.08451300e-03\n -3.60475600e-03  4.91245720e-02  3.65207830e-02  8.46474100e-03].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "input_query = input('Ask me a question: ')\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "    print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
    "\n",
    "instruction_prompt = (\n",
    "    \"You are a helpful chatbot.\\n\"\n",
    "    \"Use only the following pieces of context to answer the question. \"\n",
    "    \"Don't make up any new information:\\n\"\n",
    "    + \"\\n\".join([f' - {chunk}' for chunk, _ in retrieved_knowledge])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5306d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
